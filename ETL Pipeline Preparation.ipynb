{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline Preparation\n",
    "Follow the instructions below to help you create your ETL pipeline.\n",
    "### 1. Import libraries and load datasets.\n",
    "- Import Python libraries\n",
    "- Load `messages.csv` into a dataframe and inspect the first few lines.\n",
    "- Load `categories.csv` into a dataframe and inspect the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load messages dataset\n",
    "messages = pd.read_csv('messages.csv')\n",
    "messages.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.drop(['original'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load categories dataset\n",
    "categories = pd.read_csv('categories.csv')\n",
    "categories.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column names using list comprehensions taken from the categories.categories Series\n",
    "cols = [i.split('-')[0] for i in categories.iloc[0, 1:2].str.split(\";\")[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the first element of the categories.categories gets assigned to a new column name\n",
    "# the new column name is the first element of cols list cols[0] = 'related'\n",
    "categories[cols[0]] = categories.categories.apply(lambda x: x.split(\";\")[0].split('-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a loop to create columns with the right data for every category found in the category.category Series\n",
    "import time\n",
    "start = time.time()\n",
    "cols = [i.split('-')[0] for i in categories.iloc[0, 1:2].str.split(\";\")[0]]\n",
    "\n",
    "for col in range(0, len(cols)):\n",
    "    print(f\"Creating column {col + 1}: {cols[col]}\")\n",
    "    categories[cols[col]] = categories.categories.apply(lambda x: x.split(\";\")[col].split('-')[1]).astype('int64')\n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Calculation time: {round(stop-start, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check non binalry variables\n",
    "for col in cols:\n",
    "    val_nums = len(categories[col].value_counts()) \n",
    "    if val_nums > 2 or val_nums < 2:\n",
    "        print(f\"Column {col} has {val_nums} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like `related` and `child_alone` variables are not binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.related.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the related categories binary issue\n",
    "categories.loc[(categories['related']==2)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the child alone category which is the same and would not add any value to our model\n",
    "categories.drop(['child_alone'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.loc[(categories['id']==17919)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.loc[(categories['id']== 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories[categories.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop dupplucates\n",
    "categories.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.drop(['categories'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merge datasets.\n",
    "- Merge the messages and categories datasets using the common id\n",
    "- Assign this combined dataset to `df`, which will be cleaned in the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "df = messages.merge(categories, on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop dupplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropna\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split `categories` into separate category columns.\n",
    "- Split the values in the `categories` column on the `;` character so that each value becomes a separate column. You'll find [this method](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.Series.str.split.html) very helpful! Make sure to set `expand=True`.\n",
    "- Use the first row of categories dataframe to create column names for the categories data.\n",
    "- Rename columns of `categories` with new column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the 36 individual category columns\n",
    "categories = df.categories.str.split(pat = \";\", expand = True)\n",
    "categories.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# select the first row of the categories dataframe\n",
    "row = categories.loc[0,:]\n",
    "\n",
    "# use this row to extract a list of new column names for categories.\n",
    "# one way is to apply a lambda function that takes everything \n",
    "# up to the second to last character of each string with slicing\n",
    "row = row.str.split('-').str[0].tolist()\n",
    "cat_colnames = row\n",
    "\n",
    "# rename the columns of `categories`\n",
    "categories.columns = cat_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns of `categories`\n",
    "categories.columns = cat_colnames\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert category values to just numbers 0 or 1.\n",
    "- Iterate through the category columns in df to keep only the last character of each string (the 1 or 0). For example, `related-0` becomes `0`, `related-1` becomes `1`. Convert the string to a numeric value.\n",
    "- You can perform [normal string actions on Pandas Series](https://pandas.pydata.org/pandas-docs/stable/text.html#indexing-with-str), like indexing, by including `.str` after the Series. You may need to first convert the Series to be of type string, which you can do with `astype(str)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time        \n",
    "for col in categories:\n",
    "    # set each value to be the last character of the string\n",
    "    categories[col] = categories[col].str.split('-').str[1]  \n",
    "    # convert column from string to numeric\n",
    "    categories[col] = pd.to_numeric(categories[col])\n",
    "print(cat_colnames)\n",
    "%time\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Replace `categories` column in `df` with new category columns.\n",
    "- Drop the categories column from the df dataframe since it is no longer needed.\n",
    "- Concatenate df and categories data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the original categories column from `df`\n",
    "df.drop(['categories'], axis = 1, inplace = True)\n",
    "# no original column\n",
    "categories.loc[(categories['related']==2)] = 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the original dataframe with the new `categories` dataframe\n",
    "df = pd.concat([df, categories], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove duplicates.\n",
    "- Check how many duplicates are in this dataset.\n",
    "- Drop the duplicates.\n",
    "- Confirm duplicates were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to https://github.com/iris-theof/Disaster_response_pipeline\n",
    "def clean_data(df):\n",
    "    '''\n",
    "    Function to clean the dataframe in order to be compatible for the ML application.\n",
    "    Split the categories column with delimit ';' \n",
    "    Convert the first row values in categories dataframe to the column headers. \n",
    "    Replace the numerical values  2 to 1.\n",
    "    Drop the duplicate rows from df dataframe.\n",
    "    Drop the 'child_alone' column as it always takes the same value.\n",
    "    Drop the column 'original' as it will not be used for the ML model.\n",
    "    Drop rows with NA.\n",
    "    Remove the existing 'categories' column from the df dataframe and concat the formatted \n",
    "    categories dataframe with df dataframe.   \n",
    "    \n",
    "    Input: df\n",
    "    Output: cleaned and formatted dataframe\n",
    "    '''\n",
    "    # create a dataframe of the 36 individual category columns\n",
    "    categories = df[\"categories\"].str.split(pat=\";\",expand=True)\n",
    "    # select the first row of the categories dataframe\n",
    "    row = categories.loc[0,:]\n",
    "    # removed everything after -\n",
    "    row = row.str.split('-').str[0].tolist()\n",
    "    # use this row to extract a list of new column names for categories.                \n",
    "    category_colnames = row\n",
    "    # rename the columns of `categories`\n",
    "    categories.columns = category_colnames\n",
    "    for column in categories:\n",
    "        # set each value to be the last character of the string\n",
    "        categories[column] = categories[column].str.split('-').str[1]  \n",
    "        # convert column from string to numeric\n",
    "        categories[column] = pd.to_numeric(categories[column])\n",
    "    #replace the 2 entries by 1\n",
    "    categories.loc[(categories['related']==2)] = 1\n",
    "    categories.drop(['child_alone'], axis=1, inplace=True)\n",
    "    # drop the original categories column from `df`\n",
    "    df.drop(['categories'], axis=1, inplace=True)\n",
    "    # concatenate the original dataframe with the new `categories` dataframe\n",
    "    df = pd.concat([df, categories], axis=1)\n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # drop original column as it is not needed for the ML model\n",
    "    #df.drop(['original'], axis=1, inplace=True)\n",
    "    # drop rows with NA\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "messages_path = 'messages.csv'\n",
    "categories_path = 'categories.csv'\n",
    "\n",
    "def load_clean(messages, categories):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    messages_path = the path of the messages.csv file,\n",
    "    categories_path = the path of the categories.csv file\n",
    "    \n",
    "    Output:\n",
    "    cleaned merged dataframe using both messages and categories data\n",
    "    \n",
    "    Description:\n",
    "    Function to load and clean data in compatibility terms with the ML application\n",
    "    1. Loads data using messages and categories paths\n",
    "    3. Drops dupplicates in the messages dataframe\n",
    "    3. Drops 'original' column in the messages dataframe\n",
    "    4. Creates a list of column names which we find in the category dataframe, as category column\n",
    "    5. Loops into all reccords of the categories data frame category column to split the category \n",
    "    column and distribute new columns in the category dataframe with the respected labels\n",
    "    6. Fixes the 'related' column by replacing \"2\" to 1\n",
    "    7. Drops 'child_alone' column which has only one value, there fore would bring no value to our model\n",
    "    8. Drops categories column after having created the columns for each variable found there\n",
    "    9. Merges two dataframes\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    messages = pd.read_csv(messages_path)\n",
    "    categories = pd.read_csv(categories_path)\n",
    "    \n",
    "    # drop dupplicates in messages dataframe \n",
    "    messages.drop_duplicates(inplace = True)\n",
    "    \n",
    "    # drop original column in the messages dataframe\n",
    "    messages.drop(['original'], axis = 1, inplace = True)\n",
    "    \n",
    "    # create column names using list comprehensions taken from the categories.categories Series\n",
    "    cols = [i.split('-')[0] for i in categories.iloc[0, 1:2].str.split(\";\")[0]]\n",
    "\n",
    "    # create a loop to create columns with the right data for every category found in the category.category Series\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    for col in range(0, len(cols)):\n",
    "        print(f\"Creating column {col + 1}: {cols[col]}\")\n",
    "        categories[cols[col]] = categories.categories.apply(lambda x: x.split(\";\")[col].split('-')[1]).astype('int64')\n",
    "    stop = time.time()\n",
    "\n",
    "    print(f\"Calculation time: {round(stop-start, 2)} seconds\")\n",
    "\n",
    "    # fix the related categories binary issue\n",
    "    categories.loc[(categories['related']==2)] = 1\n",
    "\n",
    "    # drop the child alone category which is the same and would not add any value to our model\n",
    "    categories.drop(['child_alone'], axis = 1, inplace = True)\n",
    "\n",
    "    # drop categories column\n",
    "    categories.drop(['categories'], axis = 1, inplace = True)\n",
    "\n",
    "    # drop dupplicates in messages dataframe \n",
    "    categories.drop_duplicates(inplace = True)\n",
    "\n",
    "    # merge datasets\n",
    "    df = messages.merge(categories, on='id', how = 'inner')\n",
    "    \n",
    "    # drop nan values\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating column 1: related\n",
      "Creating column 2: request\n",
      "Creating column 3: offer\n",
      "Creating column 4: aid_related\n",
      "Creating column 5: medical_help\n",
      "Creating column 6: medical_products\n",
      "Creating column 7: search_and_rescue\n",
      "Creating column 8: security\n",
      "Creating column 9: military\n",
      "Creating column 10: child_alone\n",
      "Creating column 11: water\n",
      "Creating column 12: food\n",
      "Creating column 13: shelter\n",
      "Creating column 14: clothing\n",
      "Creating column 15: money\n",
      "Creating column 16: missing_people\n",
      "Creating column 17: refugees\n",
      "Creating column 18: death\n",
      "Creating column 19: other_aid\n",
      "Creating column 20: infrastructure_related\n",
      "Creating column 21: transport\n",
      "Creating column 22: buildings\n",
      "Creating column 23: electricity\n",
      "Creating column 24: tools\n",
      "Creating column 25: hospitals\n",
      "Creating column 26: shops\n",
      "Creating column 27: aid_centers\n",
      "Creating column 28: other_infrastructure\n",
      "Creating column 29: weather_related\n",
      "Creating column 30: floods\n",
      "Creating column 31: storm\n",
      "Creating column 32: fire\n",
      "Creating column 33: earthquake\n",
      "Creating column 34: cold\n",
      "Creating column 35: other_weather\n",
      "Creating column 36: direct_report\n",
      "Calculation time: 2.33 seconds\n"
     ]
    }
   ],
   "source": [
    "df = load_clean(messages_path, categories_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save the clean dataset into an sqlite database.\n",
    "You can do this with pandas [`to_sql` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html) combined with the SQLAlchemy library. Remember to import SQLAlchemy's `create_engine` in the first cell of this notebook to use it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df.to_sql('response_table', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Use this notebook to complete `etl_pipeline.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database based on new datasets specified by the user. Alternatively, you can complete `etl_pipeline.py` in the classroom on the `Project Workspace IDE` coming later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
