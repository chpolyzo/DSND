{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries to load data from the database\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def load_data(sqlite_path):\n",
    "    engine = create_engine(sqlite_path)\n",
    "    df = pd.read_sql_table('response_table',con=engine)\n",
    "    X = df.loc[:,'message'].values\n",
    "    Y = df.iloc[:,4:]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_path = 'sqlite:///C:/Users/chpol/Documents/DSND/Disaster_Response_Pipeline/data/DisasterResponse.db'\n",
    "X,Y = load_data(sqlite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookup a complicated text to work on\n",
    "- create a dictionary keeping all urls\n",
    "- look up a complicated text\n",
    "- test our code on this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chpol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\chpol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chpol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\chpol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chpol\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download(['punkt', 'words', 'stopwords', 'averaged_perceptron_tagger', 'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    a text string found in each reccord (str)\n",
    "    Output:\n",
    "    a list of stems \n",
    "    \n",
    "    Desscription:\n",
    "    Function that cterates stems - word tokens\n",
    "    1. replaces urls with the 'url' string\n",
    "    2. replaces punctuation marks with white spaces\n",
    "    3. creates lists of words out of the initial text\n",
    "    4. assigns Parts of speech to every word\n",
    "    5. reduces words to their root form by specifying verb parts of speech\n",
    "    6. reduces words to their stems - not necessary words to be understood by humans\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # regex pattern to identify an url\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # replace urls with a 'url' string\n",
    "    text = re.sub(url_regex, 'url', text)\n",
    "    # text normalization - remove punctuation and lower case\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # tokenize text to words\n",
    "    words = [w for w in word_tokenize(text) if w not in stopwords.words(\"english\")]\n",
    "    # assign \"Parts of Speech\": POS to every word - words output is a tupple\n",
    "    words = pos_tag(words)\n",
    "    # Reduce words to their root form by specifying Part of Speech: verb\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w[0], pos = 'v') for w in words]\n",
    "    # Reduce words to their stems - that is their root form not exactly a word to be understood \n",
    "    stemmed = [PorterStemmer().stem(w) for w in lemmed]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f1b97be4bf8>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f1b97be4bf8>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__bootstrap': True,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__criterion': 'gini',\n",
       " 'clf__max_depth': None,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__max_leaf_nodes': None,\n",
       " 'clf__min_impurity_decrease': 0.0,\n",
       " 'clf__min_impurity_split': None,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 10,\n",
       " 'clf__n_jobs': 1,\n",
       " 'clf__oob_score': False,\n",
       " 'clf__random_state': None,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model calculation time: 2.5681843757629395\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()/60\n",
    "# train classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "stop = time.time()/60\n",
    "print(f\"Model calculation time: {stop - start}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting time: 0.5630176812410355\n"
     ]
    }
   ],
   "source": [
    "start = time.time()/60\n",
    "y_pred = pipeline.predict(X_test)\n",
    "stop = time.time()/60\n",
    "print(f\"Predicting time: {stop - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test.values.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    plt.matshow(confusion_mat)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [ 0.  1.]\n",
      "Confusion Matrix:\n",
      " [[2671    0  334    1    1    2    0    0  138   22]\n",
      " [  13    0    8    0    0    0    0    0    0    0]\n",
      " [ 639    0  529    0    0    5    0    0  171    3]\n",
      " [  37    0   15    0    0    1    0    0   26    0]\n",
      " [  17    0    4    0    3    0    0    0   14    0]\n",
      " [  15    0    5    1    0    5    0    0   11    0]\n",
      " [   8    0    4    0    0    0    0    0    6    0]\n",
      " [   0    0    1    0    0    0    0    0    0    0]\n",
      " [ 141    0   21    0    0    0    0    0  293    2]\n",
      " [  29    0    5    0    0    0    0    0    5    0]]\n",
      "Accuracy: request                   0.878025\n",
      "offer                     0.995966\n",
      "aid_related               0.730311\n",
      "medical_help              0.922973\n",
      "medical_products          0.948905\n",
      "search_and_rescue         0.971571\n",
      "security                  0.983096\n",
      "military                  0.967345\n",
      "water                     0.945832\n",
      "food                      0.916827\n",
      "shelter                   0.930081\n",
      "clothing                  0.985017\n",
      "money                     0.978678\n",
      "missing_people            0.987322\n",
      "refugees                  0.965232\n",
      "death                     0.957549\n",
      "other_aid                 0.868229\n",
      "infrastructure_related    0.932962\n",
      "transport                 0.954476\n",
      "buildings                 0.955052\n",
      "electricity               0.983096\n",
      "tools                     0.995198\n",
      "hospitals                 0.989243\n",
      "shops                     0.995966\n",
      "aid_centers               0.985209\n",
      "other_infrastructure      0.955628\n",
      "weather_related           0.838264\n",
      "floods                    0.931809\n",
      "storm                     0.922973\n",
      "fire                      0.989819\n",
      "earthquake                0.954284\n",
      "cold                      0.980407\n",
      "other_weather             0.945255\n",
      "direct_report             0.845755\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1b840c20f0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f1b97be4bf8>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f1b97be4bf8>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__bootstrap': True,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__criterion': 'gini',\n",
       " 'clf__max_depth': None,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__max_leaf_nodes': None,\n",
       " 'clf__min_impurity_decrease': 0.0,\n",
       " 'clf__min_impurity_split': None,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 10,\n",
       " 'clf__n_jobs': 1,\n",
       " 'clf__oob_score': False,\n",
       " 'clf__random_state': None,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    parameters = {\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__n_estimators': [10, 20, 30]\n",
    "    }\n",
    "\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()/60\n",
    "\n",
    "model = build_model()\n",
    "model.fit(X_train, y_train)\n",
    "stop = time.time()/60\n",
    "print(f\"Model calculation time: {stop - start}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(cv, y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test.values.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    plt.matshow(confusion_mat)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nBest Parameters:\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "    \n",
    "display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('starting_verb', StartingVerbExtractor()),\n",
    "        ('clf', svm.SVM())\n",
    "    ])\n",
    "\n",
    "    parameters = {\n",
    "        'clf__n_estimators': [50, 100, 200],\n",
    "        'clf__min_samples_split': [2, 3, 4]\n",
    "    }\n",
    "\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()/60\n",
    "\n",
    "model = build_model()\n",
    "model.fit(X_train, y_train)\n",
    "stop = time.time()/60\n",
    "print(f\"Model calculation time: {stop - start}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('response_table',con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('response_table',con=engine)\n",
    "X = df.loc[:,'message'].values\n",
    "Y = df.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex pattern to identify an url\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "# a dictionaly comprehension keeping all indexes and urls in case we wishesd to furtehr process later\n",
    "urls_dict = \\\n",
    "{df[df.message==text].index[0]:re.findall(url_regex, text) for text in df.message if len(re.findall(url_regex, text))>1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup check out a complicated text to work on\n",
    "text = df.message[12409]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the above mentioned text to get the index od the data frame, which will later help us identify urls \n",
    "# and where these are located in the dataframe - their index\n",
    "df[df.message == 'Wind 16.0 mph NNE. Barometer 982.09 mb, gust 31.0, Temp 56.9 &amp;deg;F. Rain 0.00 in. Humidity 95% hurricane cam http://t.co/Sq2ekENuWind 16.0 mph NNE. Barometer 981.68 mb, gust 26.0, Temp 57.1 &amp;deg;F. Rain 0.00 in. Humidity 95% hurricane cam http://t.co/Sq2ekENu'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index in pandas is basic object storing axis labels for all pandas objects - we need the index value\n",
    "type(df[df.message == 'Wind 16.0 mph NNE. Barometer 982.09 mb, gust 31.0, Temp 56.9 &amp;deg;F. Rain 0.00 in. Humidity 95% hurricane cam http://t.co/Sq2ekENuWind 16.0 mph NNE. Barometer 981.68 mb, gust 26.0, Temp 57.1 &amp;deg;F. Rain 0.00 in. Humidity 95% hurricane cam http://t.co/Sq2ekENu'].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all url found in each text with the string \"url\"\n",
    "text = re.sub(url_regex, 'url', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize with lower case\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(['punkt', 'words', 'stopwords', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "words = [w for w in word_tokenize(text) if w not in stopwords.words(\"english\")]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \"Parts of Speech\": POS and Name Entity Recognition NER\n",
    "from nltk import pos_tag, ne_chunk\n",
    "words = pos_tag(words)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w[0], pos = 'v') for w in words]\n",
    "lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in lemmed]\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to https://github.com/iris-theof/Disaster_response_pipeline\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    Function that splits text into words and return the root form of the words\n",
    "    after removing the stop words\n",
    "    \n",
    "    Input: text(str): the message\n",
    "    Output: lemm(list of str): a list of the root form of the message words\n",
    "    '''\n",
    "    #Regex to find urls\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    # Finds all urls from the provided text\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    \n",
    "    #Replaces all urls found with the \"urlplaceholder\"\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "        \n",
    "    # Normalize text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())    \n",
    "        \n",
    "    # Extracts the word tokens from the provided text    \n",
    "    tokens = word_tokenize(text)\n",
    "      \n",
    "    # Remove stop words\n",
    "    stop = stopwords.words(\"english\")\n",
    "    words = [t for t in tokens if t not in stop]\n",
    "    \n",
    "    #Lemmanitizer to remove inflectional and derivationally related forms of a word\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Makes a list of clean tokens\n",
    "    clean_tokens = []\n",
    "    for tok in words:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download(['punkt', 'words', 'stopwords', 'averaged_perceptron_tagger', 'wordnet'])\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    a text string found in each reccord (str)\n",
    "    Output:\n",
    "    a list of stems \n",
    "    \n",
    "    Desscription:\n",
    "    Function that cterates stems - word tokens\n",
    "    1. replaces urls with the 'url' string\n",
    "    2. replaces punctuation marks with white spaces\n",
    "    3. creates lists of words out of the initial text\n",
    "    4. assigns Parts of speech to every word\n",
    "    5. reduces words to their root form by specifying verb parts of speech\n",
    "    6. reduces words to their stems - not necessary words to be understood by humans\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # regex pattern to identify an url\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # replace urls with a 'url' string\n",
    "    text = re.sub(url_regex, 'url', text)\n",
    "    # text normalization - remove punctuation and lower case\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # tokenize text to words\n",
    "    words = [w for w in word_tokenize(text) if w not in stopwords.words(\"english\")]\n",
    "    # assign \"Parts of Speech\": POS to every word - words output is a tupple\n",
    "    words = pos_tag(words)\n",
    "    # Reduce words to their root form by specifying Part of Speech: verb\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w[0], pos = 'v') for w in words]\n",
    "    # Reduce words to their stems - that is their root form not exactly a word to be understood \n",
    "    stemmed = [PorterStemmer().stem(w) for w in lemmed]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.message[12409]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenize(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `CountVectorizer` (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate transformers and classifier\n",
    "vect = CountVectorizer(tokenizer=tokenize)\n",
    "tfidf = TfidfTransformer()\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Fit and/or transform each to the data\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "clf.fit(X_train_tfidf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse matrix to numpy array to view\n",
    "X_train_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# initialize tf-idf vectorizer object\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect = vectorizer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data\n",
    "X_test_counts = vect.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "\n",
    "# Predict test labels\n",
    "y_pred = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tf-idf transformer object\n",
    "transformer = TfidfTransformer(smooth_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use counts from count vectorizer results to compute tf-idf values\n",
    "tfidf = transformer.fit_transform(X_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse matrix to numpy array to view\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TfidfVectorizer` = `CountVectorizer` + `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# initialize tf-idf vectorizer object\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute bag of word counts and tf-idf values\n",
    "X_vect = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse matrix to numpy array to view\n",
    "X_vect.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_vect, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(GradientBoostingClassifier(max_depth=6)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose parameters\n",
    "parameters = {'clf__estimator__n_estimators': [100, 140]}\n",
    "\n",
    "    # create grid search object\n",
    "model = GridSearchCV(pipeline, param_grid=parameters, scoring='recall_micro', cv=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
