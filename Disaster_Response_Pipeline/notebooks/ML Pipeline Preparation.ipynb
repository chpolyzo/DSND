{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries to load data from the database\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def load_data(sqlite_path):\n",
    "    engine = create_engine(sqlite_path)\n",
    "    df = pd.read_sql_table('response_table',con=engine)\n",
    "    X = df.loc[:,'message'].values\n",
    "    Y = df.iloc[:,4:]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_path = 'sqlite:///DisasterResponse.db'\n",
    "X,Y = load_data(sqlite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookup a complicated text to work on\n",
    "- create a dictionary keeping all urls\n",
    "- look up a complicated text\n",
    "- test our code on this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download(['punkt', 'words', 'stopwords', 'averaged_perceptron_tagger', 'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    a text string found in each reccord (str)\n",
    "    Output:\n",
    "    a list of stems \n",
    "    \n",
    "    Desscription:\n",
    "    Function that cterates stems - word tokens\n",
    "    1. replaces urls with the 'url' string\n",
    "    2. replaces punctuation marks with white spaces\n",
    "    3. creates lists of words out of the initial text\n",
    "    4. assigns Parts of speech to every word\n",
    "    5. reduces words to their root form by specifying verb parts of speech\n",
    "    6. reduces words to their stems - not necessary words to be understood by humans\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # regex pattern to identify an url\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # replace urls with a 'url' string\n",
    "    text = re.sub(url_regex, 'url', text)\n",
    "    # text normalization - remove punctuation and lower case\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # tokenize text to words\n",
    "    words = [w for w in word_tokenize(text) if w not in stopwords.words(\"english\")]\n",
    "    # assign \"Parts of Speech\": POS to every word - words output is a tupple\n",
    "    words = pos_tag(words)\n",
    "    # Reduce words to their root form by specifying Part of Speech: verb\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w[0], pos = 'v') for w in words]\n",
    "    # Reduce words to their stems - that is their root form not exactly a word to be understood \n",
    "    stemmed = [PorterStemmer().stem(w) for w in lemmed]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_n_train(X, Y):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    multi-labels Y which is a dataframe holding information about our 34 categories to classify,\n",
    "    a numpy array X keeping text to be classified\n",
    "    a tokenizer function to tokenize our text\n",
    "    \n",
    "    output:\n",
    "    a trained classification model\n",
    "    X_train: 60% of the X  array for trainning purposes\n",
    "    X_test: remaining 40% of the X array for testing purposes\n",
    "    y_train: 60% rows of the Y dataframe to train our classifier\n",
    "    y_test: 40% remaining \n",
    "    ])\n",
    "    \n",
    "    Description \n",
    "    splits and trains the classifier\n",
    "    \"\"\"\n",
    "    #split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "    start = time.time()/60\n",
    "    # train classifier\n",
    "    fitted = pipeline.fit(X_train, y_train)\n",
    "    stop = time.time()/60\n",
    "    print(f\"Model calculation time: {round(stop - start)} minutes\") \n",
    "    \n",
    "    return fitted, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model calculation time: 2 minutes\n"
     ]
    }
   ],
   "source": [
    "fitted_model, X_train, X_test, y_train, y_test = split_n_train(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['memory',\n",
       " 'steps',\n",
       " 'vect',\n",
       " 'tfidf',\n",
       " 'clf',\n",
       " 'vect__analyzer',\n",
       " 'vect__binary',\n",
       " 'vect__decode_error',\n",
       " 'vect__dtype',\n",
       " 'vect__encoding',\n",
       " 'vect__input',\n",
       " 'vect__lowercase',\n",
       " 'vect__max_df',\n",
       " 'vect__max_features',\n",
       " 'vect__min_df',\n",
       " 'vect__ngram_range',\n",
       " 'vect__preprocessor',\n",
       " 'vect__stop_words',\n",
       " 'vect__strip_accents',\n",
       " 'vect__token_pattern',\n",
       " 'vect__tokenizer',\n",
       " 'vect__vocabulary',\n",
       " 'tfidf__norm',\n",
       " 'tfidf__smooth_idf',\n",
       " 'tfidf__sublinear_tf',\n",
       " 'tfidf__use_idf',\n",
       " 'clf__bootstrap',\n",
       " 'clf__class_weight',\n",
       " 'clf__criterion',\n",
       " 'clf__max_depth',\n",
       " 'clf__max_features',\n",
       " 'clf__max_leaf_nodes',\n",
       " 'clf__min_impurity_decrease',\n",
       " 'clf__min_impurity_split',\n",
       " 'clf__min_samples_leaf',\n",
       " 'clf__min_samples_split',\n",
       " 'clf__min_weight_fraction_leaf',\n",
       " 'clf__n_estimators',\n",
       " 'clf__n_jobs',\n",
       " 'clf__oob_score',\n",
       " 'clf__random_state',\n",
       " 'clf__verbose',\n",
       " 'clf__warm_start']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the params of the fitted model\n",
    "[key for key in fitted_model.get_params().keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting time: 0.6064376123249531\n"
     ]
    }
   ],
   "source": [
    "start = time.time()/60\n",
    "y_pred = pipeline.predict(X_test)\n",
    "stop = time.time()/60\n",
    "print(f\"Predicting time: {stop - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(X_test, fitted_model):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    X_test = 60% of the X array to test our model\n",
    "    The fitted model we have trained on our classifier\n",
    "    \n",
    "    output:\n",
    "    y_pred = predicted outputs that indicate what\n",
    "    kind of request each text is refering to our\n",
    "    of our 34 categories we train\n",
    "    \n",
    "    Descriprion:\n",
    "    takes a trained model and aplies the test dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()/60\n",
    "    y_pred = fitted_model.predict(X_test)\n",
    "    stop = time.time()/60\n",
    "    print(f\"Model evaluation time: {round(stop - start)} minutes\") \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation time: 1 minutes\n"
     ]
    }
   ],
   "source": [
    "y_pred = eval_model(X_test, fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_test, y_pred, fitted_model, *cvd):\n",
    "    '''\n",
    "    input:\n",
    "    y_pred = predicted outputs that indicate what\n",
    "    kind of request each text is refering to our\n",
    "    of our 34 categories we train\n",
    "    \n",
    "    output:\n",
    "    displays the accuracy of each predictor of our classifier\n",
    "    displays Pipeline Parameters\n",
    "    displays the best parameters for a model run with Grid Search\n",
    "    \n",
    "    '''\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    \n",
    "    print(\"Accuracy for each predictor:\")\n",
    "    print(accuracy)\n",
    "    \n",
    "    # get the params of the fitted model\n",
    "    print(\"Get Pipeline parameters\")\n",
    "    for key in fitted_model.get_params().keys():\n",
    "        print(key)\n",
    "        \n",
    "    # check best parameters in cross validated models\n",
    "    for cv in cvd:\n",
    "        print(\"\\nBest Parameters:\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each predictor:\n",
      "request                   0.880330\n",
      "offer                     0.995390\n",
      "aid_related               0.726373\n",
      "medical_help              0.922301\n",
      "medical_products          0.952363\n",
      "search_and_rescue         0.973588\n",
      "security                  0.982328\n",
      "military                  0.966769\n",
      "water                     0.953323\n",
      "food                      0.928064\n",
      "shelter                   0.928064\n",
      "clothing                  0.987706\n",
      "money                     0.975605\n",
      "missing_people            0.987803\n",
      "refugees                  0.966673\n",
      "death                     0.958125\n",
      "other_aid                 0.867172\n",
      "infrastructure_related    0.935363\n",
      "transport                 0.956685\n",
      "buildings                 0.951498\n",
      "electricity               0.978294\n",
      "tools                     0.993853\n",
      "hospitals                 0.988283\n",
      "shops                     0.995582\n",
      "aid_centers               0.988859\n",
      "other_infrastructure      0.956685\n",
      "weather_related           0.845947\n",
      "floods                    0.940069\n",
      "storm                     0.928448\n",
      "fire                      0.989531\n",
      "earthquake                0.951402\n",
      "cold                      0.978967\n",
      "other_weather             0.947176\n",
      "direct_report             0.842009\n",
      "dtype: float64\n",
      "Get Pipeline parameters\n",
      "memory\n",
      "steps\n",
      "vect\n",
      "tfidf\n",
      "clf\n",
      "vect__analyzer\n",
      "vect__binary\n",
      "vect__decode_error\n",
      "vect__dtype\n",
      "vect__encoding\n",
      "vect__input\n",
      "vect__lowercase\n",
      "vect__max_df\n",
      "vect__max_features\n",
      "vect__min_df\n",
      "vect__ngram_range\n",
      "vect__preprocessor\n",
      "vect__stop_words\n",
      "vect__strip_accents\n",
      "vect__token_pattern\n",
      "vect__tokenizer\n",
      "vect__vocabulary\n",
      "tfidf__norm\n",
      "tfidf__smooth_idf\n",
      "tfidf__sublinear_tf\n",
      "tfidf__use_idf\n",
      "clf__bootstrap\n",
      "clf__class_weight\n",
      "clf__criterion\n",
      "clf__max_depth\n",
      "clf__max_features\n",
      "clf__max_leaf_nodes\n",
      "clf__min_impurity_decrease\n",
      "clf__min_impurity_split\n",
      "clf__min_samples_leaf\n",
      "clf__min_samples_split\n",
      "clf__min_weight_fraction_leaf\n",
      "clf__n_estimators\n",
      "clf__n_jobs\n",
      "clf__oob_score\n",
      "clf__random_state\n",
      "clf__verbose\n",
      "clf__warm_start\n"
     ]
    }
   ],
   "source": [
    "display_results(y_test, y_pred, fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    no input\n",
    "    \n",
    "    output:\n",
    "    improoved model\n",
    "    \n",
    "    Description:\n",
    "    a cross validated fitted model with improved parameters using GridSearch.\n",
    "    In our case the number of trees in the forest.\n",
    "\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    parameters = {'clf__n_estimators': [10, 50, 100]}\n",
    "\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validated fitted model time: 35 minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()/60\n",
    "cross_validated = build_model()\n",
    "cross_validated.fit(X_train, y_train)\n",
    "stop = time.time()/60\n",
    "print(f\"Cross validated fitted model time: {round(stop - start)} minutes\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation time: 1 minutes\n"
     ]
    }
   ],
   "source": [
    "y_pred_cv = eval_model(X_test, cross_validated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each predictor:\n",
      "request                   0.891663\n",
      "offer                     0.995390\n",
      "aid_related               0.756339\n",
      "medical_help              0.922397\n",
      "medical_products          0.951306\n",
      "search_and_rescue         0.973684\n",
      "security                  0.982424\n",
      "military                  0.967249\n",
      "water                     0.950442\n",
      "food                      0.928544\n",
      "shelter                   0.929793\n",
      "clothing                  0.986650\n",
      "money                     0.975797\n",
      "missing_people            0.987899\n",
      "refugees                  0.966673\n",
      "death                     0.958317\n",
      "other_aid                 0.867941\n",
      "infrastructure_related    0.935075\n",
      "transport                 0.956589\n",
      "buildings                 0.950730\n",
      "electricity               0.978390\n",
      "tools                     0.993853\n",
      "hospitals                 0.988283\n",
      "shops                     0.995486\n",
      "aid_centers               0.988859\n",
      "other_infrastructure      0.956589\n",
      "weather_related           0.860642\n",
      "floods                    0.944487\n",
      "storm                     0.927680\n",
      "fire                      0.989531\n",
      "earthquake                0.956300\n",
      "cold                      0.979159\n",
      "other_weather             0.947368\n",
      "direct_report             0.854783\n",
      "dtype: float64\n",
      "Get Pipeline parameters\n",
      "memory\n",
      "steps\n",
      "vect\n",
      "tfidf\n",
      "clf\n",
      "vect__analyzer\n",
      "vect__binary\n",
      "vect__decode_error\n",
      "vect__dtype\n",
      "vect__encoding\n",
      "vect__input\n",
      "vect__lowercase\n",
      "vect__max_df\n",
      "vect__max_features\n",
      "vect__min_df\n",
      "vect__ngram_range\n",
      "vect__preprocessor\n",
      "vect__stop_words\n",
      "vect__strip_accents\n",
      "vect__token_pattern\n",
      "vect__tokenizer\n",
      "vect__vocabulary\n",
      "tfidf__norm\n",
      "tfidf__smooth_idf\n",
      "tfidf__sublinear_tf\n",
      "tfidf__use_idf\n",
      "clf__bootstrap\n",
      "clf__class_weight\n",
      "clf__criterion\n",
      "clf__max_depth\n",
      "clf__max_features\n",
      "clf__max_leaf_nodes\n",
      "clf__min_impurity_decrease\n",
      "clf__min_impurity_split\n",
      "clf__min_samples_leaf\n",
      "clf__min_samples_split\n",
      "clf__min_weight_fraction_leaf\n",
      "clf__n_estimators\n",
      "clf__n_jobs\n",
      "clf__oob_score\n",
      "clf__random_state\n",
      "clf__verbose\n",
      "clf__warm_start\n",
      "\n",
      "Best Parameters: {'clf__n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "display_results(y_test, y_pred_cv, fitted_model, cross_validated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ada = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('count_vectorizer', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf_transformer', TfidfTransformer())\n",
    "            ]))\n",
    "            \n",
    "        ])),\n",
    "\n",
    "        ('classifier', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model calculation time: 3 minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()/60\n",
    "pipeline_ada.fit(X_train, y_train)\n",
    "stop = time.time()/60\n",
    "print(f\"Model calculation time: {round(stop - start)} minutes\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_ada.get_params().get('classifier__estimator__learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation time: 1 minutes\n"
     ]
    }
   ],
   "source": [
    "y_pred_ada = eval_model(X_test, pipeline_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each predictor:\n",
      "request                   0.889262\n",
      "offer                     0.993853\n",
      "aid_related               0.764983\n",
      "medical_help              0.931041\n",
      "medical_products          0.953995\n",
      "search_and_rescue         0.974453\n",
      "security                  0.980215\n",
      "military                  0.969842\n",
      "water                     0.961679\n",
      "food                      0.944679\n",
      "shelter                   0.944295\n",
      "clothing                  0.989147\n",
      "money                     0.978582\n",
      "missing_people            0.986938\n",
      "refugees                  0.969554\n",
      "death                     0.966673\n",
      "other_aid                 0.866884\n",
      "infrastructure_related    0.934595\n",
      "transport                 0.960334\n",
      "buildings                 0.960334\n",
      "electricity               0.979735\n",
      "tools                     0.993085\n",
      "hospitals                 0.986650\n",
      "shops                     0.994814\n",
      "aid_centers               0.987514\n",
      "other_infrastructure      0.953515\n",
      "weather_related           0.880811\n",
      "floods                    0.957261\n",
      "storm                     0.941414\n",
      "fire                      0.988667\n",
      "earthquake                0.970035\n",
      "cold                      0.983673\n",
      "other_weather             0.947176\n",
      "direct_report             0.849405\n",
      "dtype: float64\n",
      "Get Pipeline parameters\n",
      "memory\n",
      "steps\n",
      "features\n",
      "classifier\n",
      "features__n_jobs\n",
      "features__transformer_list\n",
      "features__transformer_weights\n",
      "features__text_pipeline\n",
      "features__text_pipeline__memory\n",
      "features__text_pipeline__steps\n",
      "features__text_pipeline__count_vectorizer\n",
      "features__text_pipeline__tfidf_transformer\n",
      "features__text_pipeline__count_vectorizer__analyzer\n",
      "features__text_pipeline__count_vectorizer__binary\n",
      "features__text_pipeline__count_vectorizer__decode_error\n",
      "features__text_pipeline__count_vectorizer__dtype\n",
      "features__text_pipeline__count_vectorizer__encoding\n",
      "features__text_pipeline__count_vectorizer__input\n",
      "features__text_pipeline__count_vectorizer__lowercase\n",
      "features__text_pipeline__count_vectorizer__max_df\n",
      "features__text_pipeline__count_vectorizer__max_features\n",
      "features__text_pipeline__count_vectorizer__min_df\n",
      "features__text_pipeline__count_vectorizer__ngram_range\n",
      "features__text_pipeline__count_vectorizer__preprocessor\n",
      "features__text_pipeline__count_vectorizer__stop_words\n",
      "features__text_pipeline__count_vectorizer__strip_accents\n",
      "features__text_pipeline__count_vectorizer__token_pattern\n",
      "features__text_pipeline__count_vectorizer__tokenizer\n",
      "features__text_pipeline__count_vectorizer__vocabulary\n",
      "features__text_pipeline__tfidf_transformer__norm\n",
      "features__text_pipeline__tfidf_transformer__smooth_idf\n",
      "features__text_pipeline__tfidf_transformer__sublinear_tf\n",
      "features__text_pipeline__tfidf_transformer__use_idf\n",
      "classifier__estimator__algorithm\n",
      "classifier__estimator__base_estimator\n",
      "classifier__estimator__learning_rate\n",
      "classifier__estimator__n_estimators\n",
      "classifier__estimator__random_state\n",
      "classifier__estimator\n",
      "classifier__n_jobs\n"
     ]
    }
   ],
   "source": [
    "display_results(y_test, y_pred_ada, pipeline_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = 'classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def\n",
    "with open(model_filepath, 'wb') as f:\n",
    "        pickle.dump(pipeline_ada, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Model calculation time: 3 minutes\n",
      "Model evaluation time: 1 minutes\n",
      "Accuracy for each predictor:\n",
      "request                   0.888206\n",
      "offer                     0.994333\n",
      "aid_related               0.759124\n",
      "medical_help              0.923838\n",
      "medical_products          0.953035\n",
      "search_and_rescue         0.973780\n",
      "security                  0.980599\n",
      "military                  0.972724\n",
      "water                     0.961967\n",
      "food                      0.948041\n",
      "shelter                   0.944583\n",
      "clothing                  0.989147\n",
      "money                     0.977142\n",
      "missing_people            0.987706\n",
      "refugees                  0.966865\n",
      "death                     0.961199\n",
      "other_aid                 0.864387\n",
      "infrastructure_related    0.930081\n",
      "transport                 0.956204\n",
      "buildings                 0.955532\n",
      "electricity               0.980887\n",
      "tools                     0.993277\n",
      "hospitals                 0.986458\n",
      "shops                     0.994526\n",
      "aid_centers               0.986074\n",
      "other_infrastructure      0.951402\n",
      "weather_related           0.869478\n",
      "floods                    0.956493\n",
      "storm                     0.935171\n",
      "fire                      0.989819\n",
      "earthquake                0.972051\n",
      "cold                      0.982040\n",
      "other_weather             0.945159\n",
      "direct_report             0.849885\n",
      "dtype: float64\n",
      "Get Pipeline parameters\n",
      "memory\n",
      "steps\n",
      "features\n",
      "classifier\n",
      "features__n_jobs\n",
      "features__transformer_list\n",
      "features__transformer_weights\n",
      "features__text_pipeline\n",
      "features__text_pipeline__memory\n",
      "features__text_pipeline__steps\n",
      "features__text_pipeline__count_vectorizer\n",
      "features__text_pipeline__tfidf_transformer\n",
      "features__text_pipeline__count_vectorizer__analyzer\n",
      "features__text_pipeline__count_vectorizer__binary\n",
      "features__text_pipeline__count_vectorizer__decode_error\n",
      "features__text_pipeline__count_vectorizer__dtype\n",
      "features__text_pipeline__count_vectorizer__encoding\n",
      "features__text_pipeline__count_vectorizer__input\n",
      "features__text_pipeline__count_vectorizer__lowercase\n",
      "features__text_pipeline__count_vectorizer__max_df\n",
      "features__text_pipeline__count_vectorizer__max_features\n",
      "features__text_pipeline__count_vectorizer__min_df\n",
      "features__text_pipeline__count_vectorizer__ngram_range\n",
      "features__text_pipeline__count_vectorizer__preprocessor\n",
      "features__text_pipeline__count_vectorizer__stop_words\n",
      "features__text_pipeline__count_vectorizer__strip_accents\n",
      "features__text_pipeline__count_vectorizer__token_pattern\n",
      "features__text_pipeline__count_vectorizer__tokenizer\n",
      "features__text_pipeline__count_vectorizer__vocabulary\n",
      "features__text_pipeline__tfidf_transformer__norm\n",
      "features__text_pipeline__tfidf_transformer__smooth_idf\n",
      "features__text_pipeline__tfidf_transformer__sublinear_tf\n",
      "features__text_pipeline__tfidf_transformer__use_idf\n",
      "classifier__estimator__algorithm\n",
      "classifier__estimator__base_estimator\n",
      "classifier__estimator__learning_rate\n",
      "classifier__estimator__n_estimators\n",
      "classifier__estimator__random_state\n",
      "classifier__estimator\n",
      "classifier__n_jobs\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries to load data from the database\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# import necessary libraries\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download(['punkt', 'words', 'stopwords', 'averaged_perceptron_tagger', 'wordnet'])\n",
    "\n",
    "#import necessary libraries\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def load_data(sqlite_path):\n",
    "    '''\n",
    "    Input: \n",
    "    Database filepath\n",
    "    Output: \n",
    "    multi-labels Y which is a dataframe holding information about our 34 categories to classify,\n",
    "    a numpy array X keeping text to be classified\n",
    "    \n",
    "    Description:\n",
    "    Provides curated data from sql database to X numpy.arrary and y dataframe multiclass variables\n",
    "    '''\n",
    "    engine = create_engine(sqlite_path)\n",
    "    df = pd.read_sql_table('response_table',con=engine)\n",
    "    X = df.loc[:,'message'].values\n",
    "    Y = df.iloc[:,4:]\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    a text string found in each reccord (str)\n",
    "    Output:\n",
    "    a list of stems \n",
    "    \n",
    "    Desscription:\n",
    "    Function that cterates stems - word tokens\n",
    "    1. replaces urls with the 'url' string\n",
    "    2. replaces punctuation marks with white spaces\n",
    "    3. creates lists of words out of the initial text\n",
    "    4. assigns Parts of speech to every word\n",
    "    5. reduces words to their root form by specifying verb parts of speech\n",
    "    6. reduces words to their stems - not necessary words to be understood by humans\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # regex pattern to identify an url\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # replace urls with a 'url' string\n",
    "    text = re.sub(url_regex, 'url', text)\n",
    "    # text normalization - remove punctuation and lower case\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # tokenize text to words\n",
    "    words = [w for w in word_tokenize(text) if w not in stopwords.words(\"english\")]\n",
    "    # assign \"Parts of Speech\": POS to every word - words output is a tupple\n",
    "    words = pos_tag(words)\n",
    "    # Reduce words to their root form by specifying Part of Speech: verb\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w[0], pos = 'v') for w in words]\n",
    "    # Reduce words to their stems - that is their root form not exactly a word to be understood \n",
    "    stemmed = [PorterStemmer().stem(w) for w in lemmed]\n",
    "    \n",
    "    return stemmed\n",
    "\n",
    "def split_n_train(X, Y):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    multi-labels Y which is a dataframe holding information about our 34 categories to classify,\n",
    "    a numpy array X keeping text to be classified\n",
    "    a tokenizer function to tokenize our text\n",
    "    \n",
    "    output:\n",
    "    a trained classification model\n",
    "    X_train: 60% of the X  array for trainning purposes\n",
    "    X_test: remaining 40% of the X array for testing purposes\n",
    "    y_train: 60% rows of the Y dataframe to train our classifier\n",
    "    y_test: 40% remaining \n",
    "    ])\n",
    "    \n",
    "    Description \n",
    "    splits and trains the classifier\n",
    "    \"\"\"\n",
    "    #split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('count_vectorizer', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf_transformer', TfidfTransformer())\n",
    "            ]))\n",
    "            \n",
    "        ])),\n",
    "\n",
    "        ('classifier', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "    ])\n",
    "    start = time.time()/60\n",
    "    # train classifier\n",
    "    fitted = pipeline.fit(X_train, y_train)\n",
    "    stop = time.time()/60\n",
    "    print(f\"Model calculation time: {round(stop - start)} minutes\") \n",
    "    \n",
    "    return fitted, X_train, X_test, y_train, y_test\n",
    "\n",
    "def eval_model(X_test, fitted_model):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    X_test = 60% of the X array to test our model\n",
    "    The fitted model we have trained on our classifier\n",
    "    \n",
    "    output:\n",
    "    y_pred = predicted outputs that indicate what\n",
    "    kind of request each text is refering to our\n",
    "    of our 34 categories we train\n",
    "    \n",
    "    Descriprion:\n",
    "    takes a trained model and aplies the test dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()/60\n",
    "    y_pred = fitted_model.predict(X_test)\n",
    "    stop = time.time()/60\n",
    "    print(f\"Model evaluation time: {round(stop - start)} minutes\") \n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def save_model(fitted_model, model_filepath):\n",
    "    '''\n",
    "    Input:\n",
    "    the fitted model\n",
    "    model_filepath (str) is the path of the pickle file to be saved\n",
    "    '''\n",
    "\n",
    "    with open(model_filepath, 'wb') as f:\n",
    "        pickle.dump(fitted_model, f)\n",
    "        \n",
    "        \n",
    "def main():\n",
    "    sqlite_path = 'sqlite:///DisasterResponse.db'\n",
    "    X,Y = load_data(sqlite_path)\n",
    "    fitted_model, X_train, X_test, y_train, y_test = split_n_train(X, Y)\n",
    "    y_pred = eval_model(X_test, fitted_model)\n",
    "    display_results(y_test, y_pred, fitted_model)\n",
    "    save_model(fitted_model, model_filepath)\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('response_table',con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('response_table',con=engine)\n",
    "X = df.loc[:,'message'].values\n",
    "Y = df.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex pattern to identify an url\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "# a dictionaly comprehension keeping all indexes and urls in case we wishesd to furtehr process later\n",
    "urls_dict = \\\n",
    "{df[df.message==text].index[0]:re.findall(url_regex, text) for text in df.message if len(re.findall(url_regex, text))>1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup check out a complicated text to work on\n",
    "text = df.message[12409]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the above mentioned text to get the index od the data frame, which will later help us identify urls \n",
    "# and where these are located in the dataframe - their index\n",
    "df[df.message == 'Wind 16.0 mph NNE. Barometer 982.09 mb, gust 31.0, Temp 56.9 &amp;deg;F. Rain 0.00 in. Humidity 95% hurricane cam http://t.co/Sq2ekENuWind 16.0 mph NNE. Barometer 981.68 mb, gust 26.0, Temp 57.1 &amp;deg;F. Rain 0.00 in. Humidity 95% hurricane cam http://t.co/Sq2ekENu'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index in pandas is basic object storing axis labels for all pandas objects - we need the index value\n",
    "type(df[df.message == 'Wind 16.0 mph NNE. Barometer 982.09 mb, gust 31.0, Temp 56.9 &amp;deg;F. Rain 0.00 in. Humidity 95% hurricane cam http://t.co/Sq2ekENuWind 16.0 mph NNE. Barometer 981.68 mb, gust 26.0, Temp 57.1 &amp;deg;F. Rain 0.00 in. Humidity 95% hurricane cam http://t.co/Sq2ekENu'].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all url found in each text with the string \"url\"\n",
    "text = re.sub(url_regex, 'url', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize with lower case\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(['punkt', 'words', 'stopwords', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "words = [w for w in word_tokenize(text) if w not in stopwords.words(\"english\")]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \"Parts of Speech\": POS and Name Entity Recognition NER\n",
    "from nltk import pos_tag, ne_chunk\n",
    "words = pos_tag(words)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w[0], pos = 'v') for w in words]\n",
    "lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in lemmed]\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to https://github.com/iris-theof/Disaster_response_pipeline\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    Function that splits text into words and return the root form of the words\n",
    "    after removing the stop words\n",
    "    \n",
    "    Input: text(str): the message\n",
    "    Output: lemm(list of str): a list of the root form of the message words\n",
    "    '''\n",
    "    #Regex to find urls\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    # Finds all urls from the provided text\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    \n",
    "    #Replaces all urls found with the \"urlplaceholder\"\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "        \n",
    "    # Normalize text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())    \n",
    "        \n",
    "    # Extracts the word tokens from the provided text    \n",
    "    tokens = word_tokenize(text)\n",
    "      \n",
    "    # Remove stop words\n",
    "    stop = stopwords.words(\"english\")\n",
    "    words = [t for t in tokens if t not in stop]\n",
    "    \n",
    "    #Lemmanitizer to remove inflectional and derivationally related forms of a word\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Makes a list of clean tokens\n",
    "    clean_tokens = []\n",
    "    for tok in words:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download(['punkt', 'words', 'stopwords', 'averaged_perceptron_tagger', 'wordnet'])\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    a text string found in each reccord (str)\n",
    "    Output:\n",
    "    a list of stems \n",
    "    \n",
    "    Desscription:\n",
    "    Function that cterates stems - word tokens\n",
    "    1. replaces urls with the 'url' string\n",
    "    2. replaces punctuation marks with white spaces\n",
    "    3. creates lists of words out of the initial text\n",
    "    4. assigns Parts of speech to every word\n",
    "    5. reduces words to their root form by specifying verb parts of speech\n",
    "    6. reduces words to their stems - not necessary words to be understood by humans\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # regex pattern to identify an url\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # replace urls with a 'url' string\n",
    "    text = re.sub(url_regex, 'url', text)\n",
    "    # text normalization - remove punctuation and lower case\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # tokenize text to words\n",
    "    words = [w for w in word_tokenize(text) if w not in stopwords.words(\"english\")]\n",
    "    # assign \"Parts of Speech\": POS to every word - words output is a tupple\n",
    "    words = pos_tag(words)\n",
    "    # Reduce words to their root form by specifying Part of Speech: verb\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w[0], pos = 'v') for w in words]\n",
    "    # Reduce words to their stems - that is their root form not exactly a word to be understood \n",
    "    stemmed = [PorterStemmer().stem(w) for w in lemmed]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.message[12409]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenize(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `CountVectorizer` (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate transformers and classifier\n",
    "vect = CountVectorizer(tokenizer=tokenize)\n",
    "tfidf = TfidfTransformer()\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Fit and/or transform each to the data\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "clf.fit(X_train_tfidf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse matrix to numpy array to view\n",
    "X_train_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# initialize tf-idf vectorizer object\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect = vectorizer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data\n",
    "X_test_counts = vect.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "\n",
    "# Predict test labels\n",
    "y_pred = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tf-idf transformer object\n",
    "transformer = TfidfTransformer(smooth_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use counts from count vectorizer results to compute tf-idf values\n",
    "tfidf = transformer.fit_transform(X_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse matrix to numpy array to view\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TfidfVectorizer` = `CountVectorizer` + `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# initialize tf-idf vectorizer object\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute bag of word counts and tf-idf values\n",
    "X_vect = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse matrix to numpy array to view\n",
    "X_vect.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_vect, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()/60\n",
    "# train classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "stop = time.time()/60\n",
    "print(f\"Model calculation time: {round(stop - start)} minutes\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(GradientBoostingClassifier(max_depth=6)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose parameters\n",
    "parameters = {'clf__estimator__n_estimators': [100, 140]}\n",
    "\n",
    "    # create grid search object\n",
    "model = GridSearchCV(pipeline, param_grid=parameters, scoring='recall_micro', cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "# Build a custom transformer which will extract the starting verb of a sentence\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Starting Verb Extractor class\n",
    "    \n",
    "    This class extract the starting verb of a sentence,\n",
    "    creating a new feature for the ML classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Given it is a tranformer we can return the self \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()/60\n",
    "# train classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "stop = time.time()/60\n",
    "print(f\"Model calculation time: {round(stop - start)} minutes\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
